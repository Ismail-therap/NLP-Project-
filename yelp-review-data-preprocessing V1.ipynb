{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-21T00:19:26.272510Z","iopub.execute_input":"2021-10-21T00:19:26.273112Z","iopub.status.idle":"2021-10-21T00:19:26.309723Z","shell.execute_reply.started":"2021-10-21T00:19:26.272987Z","shell.execute_reply":"2021-10-21T00:19:26.308652Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"print(os.listdir('../input'))","metadata":{"execution":{"iopub.status.busy":"2021-10-21T00:19:32.455716Z","iopub.execute_input":"2021-10-21T00:19:32.456556Z","iopub.status.idle":"2021-10-21T00:19:32.461953Z","shell.execute_reply.started":"2021-10-21T00:19:32.456505Z","shell.execute_reply":"2021-10-21T00:19:32.460866Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nbusiness_json_path = '../input/yelp-dataset/yelp_academic_dataset_business.json'\ndf_b = pd.read_json(business_json_path, lines=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T00:21:58.093429Z","iopub.execute_input":"2021-10-21T00:21:58.093894Z","iopub.status.idle":"2021-10-21T00:22:04.603298Z","shell.execute_reply.started":"2021-10-21T00:21:58.093864Z","shell.execute_reply":"2021-10-21T00:22:04.602347Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"size = 100000\nreview = pd.read_json('../input/yelp-dataset/yelp_academic_dataset_review.json', lines=True,\n                      dtype={'review_id':str,'user_id':str,\n                             'business_id':str,'stars':int,\n                             'date':str,'text':str,'useful':int,\n                             'funny':int,'cool':int},\n                      chunksize=size)\n\n\n# There are multiple chunks to be read\nchunk_list = []\nfor chunk_review in review:\n    # Drop columns that aren't needed\n    chunk_review = chunk_review.drop(['review_id','useful','funny','cool'], axis=1)\n    # Renaming column name to avoid conflict with business overall star rating\n    chunk_review = chunk_review.rename(columns={'stars': 'review_stars'})\n    # Inner merge with edited business file so only reviews related to the business remain\n    chunk_merged = pd.merge(df_b, chunk_review, on='business_id', how='inner')\n    # Show feedback on progress\n    print(f\"{chunk_merged.shape[0]} out of {size:,} related reviews\")\n    chunk_list.append(chunk_merged)\n# After trimming down the review file, concatenate all relevant data back to one dataframe\ndf = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T00:22:14.750474Z","iopub.execute_input":"2021-10-21T00:22:14.750758Z","iopub.status.idle":"2021-10-21T00:26:33.830300Z","shell.execute_reply.started":"2021-10-21T00:22:14.750727Z","shell.execute_reply":"2021-10-21T00:26:33.829454Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Analysis","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\nplt.hist(df[\"review_stars\"],weights=np.ones(len(df[\"review_stars\"])) / len(df[\"review_stars\"]))\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T00:31:45.497448Z","iopub.execute_input":"2021-10-21T00:31:45.497711Z","iopub.status.idle":"2021-10-21T00:31:45.981754Z","shell.execute_reply.started":"2021-10-21T00:31:45.497687Z","shell.execute_reply":"2021-10-21T00:31:45.980765Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"We observed that over 40% reviews are 5 star and 2 star review is only around 8%. So, we have to normalize the number of reviews intermes of given star. In other word we have to take similar amount of review from each star given. ","metadata":{}},{"cell_type":"markdown","source":"## Normalizing reviews:","metadata":{}},{"cell_type":"code","source":"reviews_smallest = df[df[\"review_stars\"] == 2]\n\n## sampling one star review:\nreviews_1 = df[df[\"review_stars\"] == 1]\nreviews_one = reviews_1.sample(n=reviews_smallest.shape[0])\n\n## sampling three star review:\nreviews_3 = df[df[\"review_stars\"] == 3]\nreviews_three = reviews_3.sample(n=reviews_smallest.shape[0])\n\n## sampling four star review:\nreviews_4 = df[df[\"review_stars\"] == 4]\nreviews_four = reviews_4.sample(n=reviews_smallest.shape[0])\n\n## sampling five star review:\nreviews_5 = df[df[\"review_stars\"] == 5]\nreviews_five = reviews_5.sample(n=reviews_smallest.shape[0])\n\nuniformed_review = reviews_smallest.append([reviews_one, reviews_three,reviews_four,reviews_five])\nuniformed_review.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-21T00:36:44.569604Z","iopub.execute_input":"2021-10-21T00:36:44.569967Z","iopub.status.idle":"2021-10-21T00:37:04.469656Z","shell.execute_reply.started":"2021-10-21T00:36:44.569932Z","shell.execute_reply":"2021-10-21T00:37:04.468774Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"So, after normalizaton we have around 3.5 million of reviews.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import PercentFormatter\nplt.hist(uniformed_review[\"review_stars\"],weights=np.ones(len(uniformed_review[\"review_stars\"])) / len(uniformed_review[\"review_stars\"]))\nplt.gca().yaxis.set_major_formatter(PercentFormatter(1))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-21T00:38:03.961100Z","iopub.execute_input":"2021-10-21T00:38:03.961421Z","iopub.status.idle":"2021-10-21T00:38:04.215521Z","shell.execute_reply.started":"2021-10-21T00:38:03.961380Z","shell.execute_reply":"2021-10-21T00:38:04.214564Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"markdown","source":"At this stage I am preprocessing texts using Tensorflow and Keras. ","metadata":{}},{"cell_type":"code","source":"X, y = (uniformed_review['text'].values, uniformed_review['review_stars'].values)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T00:38:42.964266Z","iopub.execute_input":"2021-10-21T00:38:42.964574Z","iopub.status.idle":"2021-10-21T00:38:42.968328Z","shell.execute_reply.started":"2021-10-21T00:38:42.964546Z","shell.execute_reply":"2021-10-21T00:38:42.967709Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#from keras.preprocessing.text import Tokenizer\n#from keras.preprocessing.sequence import pad_sequences\n#tk = Tokenizer(lower = True)\n#tk.fit_on_texts(X)\n#X_seq = tk.texts_to_sequences(X)\n#X_pad = pad_sequences(X_seq, maxlen=100, padding='post')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenize the text\nimport tensorflow as tf\ntokenizer = tf.keras.preprocessing.text.Tokenizer(lower=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T00:42:18.411471Z","iopub.execute_input":"2021-10-21T00:42:18.412061Z","iopub.status.idle":"2021-10-21T00:42:24.233194Z","shell.execute_reply.started":"2021-10-21T00:42:18.412021Z","shell.execute_reply":"2021-10-21T00:42:24.232455Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"tokenizer.fit_on_texts(X)\nX_seq = tokenizer.texts_to_sequences(X)","metadata":{"execution":{"iopub.status.busy":"2021-10-21T00:42:49.359063Z","iopub.execute_input":"2021-10-21T00:42:49.359355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.preprocessing.text.Tokenizer(\n    num_words=None,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n    lower=True, split=' ', char_level=False, oov_token=None,\n    document_count=0, **kwargs\n)","metadata":{"execution":{"iopub.status.busy":"2021-09-29T08:23:53.709335Z","iopub.execute_input":"2021-09-29T08:23:53.709622Z","iopub.status.idle":"2021-09-29T08:23:53.738751Z","shell.execute_reply.started":"2021-09-29T08:23:53.709594Z","shell.execute_reply":"2021-09-29T08:23:53.73775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}